{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ad5ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/hidet/utils/stack_limit.py:24: UserWarning: The hard limit for stack size is too small (10.0 MiB), we recommend to increase it to 512.0 MiB. If you are the root user on Linux OS, you could refer to `man limits.conf` to increase this limit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoModelForCausalLM, GPT2Config\n",
    "\n",
    "import hidet\n",
    "from hidet.utils import benchmark_func\n",
    "\n",
    "from transformer_deploy.utils.generative_model import GPTModelWrapper\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfdb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "model: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178f9370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensors\n",
      "{'input_ids': tensor([[ 4342,   318,   617,  2420,   284, 37773, 18435,  2159]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input tensor shape\n",
      "torch.Size([1, 8])\n",
      "output tensor\n",
      "tensor([[[ -34.3027,  -33.9891,  -37.5683,  ...,  -42.6734,  -42.0399,\n",
      "           -34.6136],\n",
      "         [ -83.3065,  -82.9769,  -86.1204,  ...,  -89.8063,  -89.4546,\n",
      "           -83.6084],\n",
      "         [ -91.4901,  -92.5656,  -95.6423,  ...,  -96.6183,  -98.1546,\n",
      "           -91.5266],\n",
      "         ...,\n",
      "         [ -92.8820,  -94.8433,  -98.9224,  ..., -101.4426, -103.2702,\n",
      "           -95.7642],\n",
      "         [ -72.6140,  -76.3407,  -79.7973,  ...,  -87.3300,  -85.7930,\n",
      "           -77.7521],\n",
      "         [-103.6147, -108.7898, -109.6276,  ..., -116.8557, -116.5565,\n",
      "          -107.4467]]])\n",
      "output shape\n",
      "torch.Size([1, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Here is some text to encode Hello World\", return_tensors=\"pt\")\n",
    "print(\"input tensors\")\n",
    "print(inputs)\n",
    "print(\"input tensor shape\")\n",
    "print(inputs[\"input_ids\"].size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(\"output tensor\")\n",
    "print(logits)\n",
    "print(\"output shape\")\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e105916",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "for k, v in input_ids.items():\n",
    "    input_ids[k] = v.type(dtype=torch.int32)\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model,\n",
    "    output_path=\"test-gpt2.onnx\",\n",
    "    inputs_pytorch=dict(input_ids),\n",
    "    quantization=False,\n",
    "    var_output_seq=True,\n",
    "    output_names=[\"output\"],\n",
    ")\n",
    "\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "num_attention_heads, hidden_size = get_model_size(path=model_name)\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-gpt2.onnx\",\n",
    "    onnx_optim_model_path=\"test-gpt2-opt.onnx\",\n",
    "    fp16=True,\n",
    "    use_cuda=True,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    architecture=\"gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563fcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -34.3028,  -33.9892,  -37.5684,  ...,  -42.6735,  -42.0400,\n",
      "           -34.6137],\n",
      "         [ -83.3065,  -82.9769,  -86.1204,  ...,  -89.8063,  -89.4546,\n",
      "           -83.6084],\n",
      "         [ -91.4901,  -92.5656,  -95.6423,  ...,  -96.6184,  -98.1545,\n",
      "           -91.5266],\n",
      "         ...,\n",
      "         [ -92.8820,  -94.8432,  -98.9224,  ..., -101.4425, -103.2702,\n",
      "           -95.7642],\n",
      "         [ -72.6140,  -76.3407,  -79.7973,  ...,  -87.3300,  -85.7930,\n",
      "           -77.7521],\n",
      "         [-103.6147, -108.7899, -109.6277,  ..., -116.8558, -116.5565,\n",
      "          -107.4467]]], device='cuda:0')\n",
      "----\n",
      "Pytorch Forward Pass: 11.7891 ms\n"
     ]
    }
   ],
   "source": [
    "def inference_torch(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids)\n",
    "    return model.lm_head(transformer_outputs.last_hidden_state)\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "inputs.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    logits = inference_torch(inputs.input_ids)\n",
    "    print(logits)\n",
    "    print('----\\nPytorch Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: inference_torch(inputs.input_ids))))\n",
    "_ = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cf11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -34.3027,  -33.9891,  -37.5683,  ...,  -42.6734,  -42.0399,\n",
      "           -34.6136],\n",
      "         [ -83.3065,  -82.9769,  -86.1204,  ...,  -89.8062,  -89.4546,\n",
      "           -83.6083],\n",
      "         [ -91.4901,  -92.5656,  -95.6423,  ...,  -96.6184,  -98.1545,\n",
      "           -91.5266],\n",
      "         ...,\n",
      "         [ -92.8820,  -94.8432,  -98.9224,  ..., -101.4425, -103.2702,\n",
      "           -95.7642],\n",
      "         [ -72.6140,  -76.3407,  -79.7973,  ...,  -87.3300,  -85.7930,\n",
      "           -77.7521],\n",
      "         [-103.6147, -108.7898, -109.6277,  ..., -116.8558, -116.5565,\n",
      "          -107.4467]]], device='cuda:0')\n",
      "----\n",
      "ONNX Forward Pass: 5.0749 ms\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "def inference_onnx(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"]\n",
    "\n",
    "inputs.to(\"cuda\")\n",
    "logits = inference_onnx(inputs.input_ids)\n",
    "print(logits)\n",
    "print('----\\nONNX Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: inference_onnx(inputs.input_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2432de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -34.3125,  -34.0000,  -37.5938,  ...,  -42.6875,  -42.0625,\n",
      "           -34.6250],\n",
      "         [ -83.2500,  -82.9375,  -86.1250,  ...,  -89.7500,  -89.4375,\n",
      "           -83.5625],\n",
      "         [ -91.5000,  -92.5625,  -95.6250,  ...,  -96.6250,  -98.1875,\n",
      "           -91.5000],\n",
      "         ...,\n",
      "         [ -92.8750,  -94.8750,  -98.9375,  ..., -101.4375, -103.3125,\n",
      "           -95.8125],\n",
      "         [ -72.6250,  -76.3750,  -79.8125,  ...,  -87.3750,  -85.8125,\n",
      "           -77.8125],\n",
      "         [-103.6875, -108.8125, -109.6875,  ..., -116.8750, -116.5625,\n",
      "          -107.5000]]], device='cuda:0')\n",
      "----\n",
      "ONNX Optimized Forward Pass: 4.5551 ms\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"]\n",
    "\n",
    "inputs.to(\"cuda\")\n",
    "logits = inference_onnx_optimized(inputs.input_ids)\n",
    "print(logits)\n",
    "print('----\\nONNX Optimized Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: inference_onnx_optimized(inputs.input_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b223e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Input names: ['input_ids']\n",
      "Output names:  ['output']\n"
     ]
    }
   ],
   "source": [
    "hidet_onnx_module = hidet.graph.frontend.from_onnx(\"test-gpt2.onnx\")\n",
    "\n",
    "print('Input names:', hidet_onnx_module.input_names)\n",
    "print('Output names: ', hidet_onnx_module.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269f5d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = hidet.from_torch(inputs.input_ids)\n",
    "logits = hidet_onnx_module(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd6133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=(1, 8, 50257), dtype='float32', device='cuda:0')\n",
      "[[[ -34.302917  -33.989315  -37.568527 ...  -42.67358   -42.04013\n",
      "    -34.613758]\n",
      "  [ -83.30651   -82.9768    -86.12037  ...  -89.80621   -89.45452\n",
      "    -83.60836 ]\n",
      "  [ -91.49005   -92.565575  -95.64226  ...  -96.61834   -98.15459\n",
      "    -91.5266  ]\n",
      "  ...\n",
      "  [ -92.88196   -94.843315  -98.922386 ... -101.44251  -103.27026\n",
      "    -95.7642  ]\n",
      "  [ -72.614     -76.340805  -79.797386 ...  -87.33003   -85.79304\n",
      "    -77.75215 ]\n",
      "  [-103.61467  -108.789795 -109.62762  ... -116.85566  -116.55652\n",
      "   -107.446655]]]\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86164542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Hidet  Forward Pass: 2084.0274 ms\n"
     ]
    }
   ],
   "source": [
    "print('----\\nHidet  Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: hidet_onnx_module(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b660245",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_data = hidet.symbol_like(data)\n",
    "symbol_output = hidet_onnx_module(symbol_data)\n",
    "graph: hidet.FlowGraph = hidet.trace_from(symbol_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafd6e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "cuda_graph = graph.cuda_graph()\n",
    "(output,) = cuda_graph.run([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40abf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=(1, 8, 50257), dtype='float32', device='cuda:0')\n",
      "[[[ -34.302917  -33.989315  -37.568527 ...  -42.67358   -42.04013\n",
      "    -34.613758]\n",
      "  [ -83.30651   -82.9768    -86.12037  ...  -89.80621   -89.45452\n",
      "    -83.60836 ]\n",
      "  [ -91.49005   -92.565575  -95.64226  ...  -96.61834   -98.15459\n",
      "    -91.5266  ]\n",
      "  ...\n",
      "  [ -92.88196   -94.843315  -98.922386 ... -101.44251  -103.27026\n",
      "    -95.7642  ]\n",
      "  [ -72.614     -76.340805  -79.797386 ...  -87.33003   -85.79304\n",
      "    -77.75215 ]\n",
      "  [-103.61467  -108.789795 -109.62762  ... -116.85566  -116.55652\n",
      "   -107.446655]]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a071e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Cuda Graph Forward Pass: 11.6329 ms\n"
     ]
    }
   ],
   "source": [
    "print('----\\nCuda Graph Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: cuda_graph.run([data]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29fd2719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidet.option.search_space(2)\n",
    "with hidet.graph.PassContext() as ctx:\n",
    "    ctx.save_graph_instrument('./outs/graphs')\n",
    "    graph_opt: hidet.FlowGraph = hidet.graph.optimize(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4224c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cuda task \u001b[92mfused(b=float32(4, 192, 2304), y=float32(768,), y=float32(768,), x=float32(1, 8, 768), y=float32(1, 8, 1), y=float32(1, 4, 8, 2304), fused_ops='div mul add reshape broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling: 100%|██████████████████████████████| 214/214 [10:47<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch build 214 modules within 647.959 seconds, on average 3.0 seconds per module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████████████████████| 214/214 [00:01<00:00, 171.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cuda task \u001b[92mfused(x=float32(1, 4, 8, 2304), y=float32(2304,), y=float32(1, 8, 2304), fused_ops='reduce_sum reshape add reshape', anchor='reduce_sum')\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cuda task \u001b[92mfused(data=float32(1, 8, 2304), y=float32(12, 8, 8, 8), fused_ops='slice reshape rearrange rearrange reshape rearrange slice reshape rearrange rearrange reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
      "Compiling cpu task \u001b[92mcast(x=float64(1, 8, 2304), y=float32(1, 8, 2304))\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mcast(x=float64(12, 8, 8, 8), y=float32(12, 8, 8, 8))\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling:  63%|██████████████████▉           | 135/214 [05:32<04:31,  3.44s/it]"
     ]
    }
   ],
   "source": [
    "cuda_graph = graph_opt.cuda_graph()\n",
    "(output,) = cuda_graph.run([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b47aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----\\nCuda Graph Forward Pass: {:.4f} ms'.format(benchmark_func(lambda: cuda_graph.run([data]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a166f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
